{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "e1wEaZKeLUF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on this paper: http://arxiv.org/abs/1508.06576"
      ]
    },
    {
      "metadata": {
        "id": "BRrE8UnSRqz9",
        "colab_type": "code",
        "outputId": "ed4a7add-1bcf-4777-8432-ce16941134a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vns8cFB-4kJE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code just gets input images, feel free to change the links etc."
      ]
    },
    {
      "metadata": {
        "id": "IeuNFC-SBUsO",
        "colab_type": "code",
        "outputId": "d48f730a-58d0-4c4d-ffd7-8d72a11267e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "!ls .\n",
        "!mkdir results\n",
        "!curl https://www1.grc.nasa.gov/wp-content/uploads/N3-X_3600x3000.jpg > plane.jpg\n",
        "!curl https://upload.wikimedia.org/wikipedia/commons/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg > starry.jpg\n",
        "!curl https://i.redd.it/o4tgbutenbn21.jpg > trippy.jpg\n",
        "!curl https://raw.githubusercontent.com/lengstrom/fast-style-transfer/master/examples/style/udnie.jpg > color.jpg\n",
        "!curl https://images.graph.cool/v1/cj6c28vh912680101ozc2paxj/cjf2y07bi0xkc0140u26s5sh3/0x0:768x624/960x960/picasso_lesfemmesdalger_low.jpg > picasso.jpg\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4234k  100 4234k    0     0  2824k      0  0:00:01  0:00:01 --:--:-- 2822k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  205M  100  205M    0     0  28.8M      0  0:00:07  0:00:07 --:--:-- 29.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  599k  100  599k    0     0  8442k      0 --:--:-- --:--:-- --:--:-- 8325k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 87874  100 87874    0     0   356k      0 --:--:-- --:--:-- --:--:--  356k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  190k  100  190k    0     0   200k      0 --:--:-- --:--:-- --:--:--  199k\n",
            "color.jpg  picasso.jpg\tplane.jpg  results  sample_data  starry.jpg  trippy.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wzLmTA0e4qFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code gets the paths the images are in, and sets up some other data used for processing"
      ]
    },
    {
      "metadata": {
        "id": "wXdf5MH9_StR",
        "colab_type": "code",
        "outputId": "fcf6ca46-6918-43e1-e7c7-a18414a63624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "base_image_path = input(\"Base image path: \")\n",
        "style_reference_image_path = input(\"Style reference image path: \")\n",
        "result_prefix = \"results/\"\n",
        "iterations = 10\n",
        "\n",
        "# these are the weights of the different loss components\n",
        "total_variation_weight = 1.0\n",
        "style_weight = 1.0\n",
        "content_weight = 0.025\n",
        "\n",
        "# dimensions of the generated picture.\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base image path: trippy.jpg\n",
            "Style reference image path: trippy.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WmGZJfLH45Z6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This is where the code for the assignment will go:\n",
        "\n",
        "To complete the assignment, implement either:\n",
        "1. Instance normalization on the input set\n",
        "2. An image transformation network as input into the vgg19 preprocess input. (This is ***hard***. See the \"Lengstrom Fast style transfer in blackboard if you're attempting to do this\")\n",
        "\n",
        "\n",
        " ## **Note:**\n",
        "Run all code in above cells before running this cell"
      ]
    },
    {
      "metadata": {
        "id": "rvl4esbZX76V",
        "colab_type": "code",
        "outputId": "1abf7938-4d27-4d45-f2c7-5dbd4554c37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((3, img_nrows, img_ncols))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "# get tensor representations of our images\n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
        "\n",
        "# this will contain our generated image\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
        "else:\n",
        "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
        "\n",
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)\n",
        "\n",
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "# compute the neural style loss\n",
        "# first we need to define 4 util functions\n",
        "\n",
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "    assert K.ndim(x) == 3\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        features = K.batch_flatten(x)\n",
        "    else:\n",
        "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram\n",
        "\n",
        "# the \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "\n",
        "\n",
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))\n",
        "\n",
        "# the 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        a = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
        "        b = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
        "    else:\n",
        "        a = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "        b = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))\n",
        "\n",
        "# combine these loss functions into a single scalar\n",
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss += content_weight * content_loss(base_image_features,\n",
        "                                      combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss += (style_weight / len(feature_layers)) * sl\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "# get the gradients of the generated image wrt the loss\n",
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)\n",
        "\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
        "    else:\n",
        "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient.\n",
        "\n",
        "\n",
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "print('Iteration %d completed in %ds' % (i, end_time - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "Start of iteration 0\n",
            "Current loss value: 6579267000.0\n",
            "Image saved as results/_at_iteration_0.png\n",
            "Start of iteration 1\n",
            "Current loss value: 5013518000.0\n",
            "Image saved as results/_at_iteration_1.png\n",
            "Start of iteration 2\n",
            "Current loss value: 4515052500.0\n",
            "Image saved as results/_at_iteration_2.png\n",
            "Start of iteration 3\n",
            "Current loss value: 4259270700.0\n",
            "Image saved as results/_at_iteration_3.png\n",
            "Start of iteration 4\n",
            "Current loss value: 4133300200.0\n",
            "Image saved as results/_at_iteration_4.png\n",
            "Start of iteration 5\n",
            "Current loss value: 4044209200.0\n",
            "Image saved as results/_at_iteration_5.png\n",
            "Start of iteration 6\n",
            "Current loss value: 3981283800.0\n",
            "Image saved as results/_at_iteration_6.png\n",
            "Start of iteration 7\n",
            "Current loss value: 3931911200.0\n",
            "Image saved as results/_at_iteration_7.png\n",
            "Start of iteration 8\n",
            "Current loss value: 3883262500.0\n",
            "Image saved as results/_at_iteration_8.png\n",
            "Start of iteration 9\n",
            "Current loss value: 3850079700.0\n",
            "Image saved as results/_at_iteration_9.png\n",
            "Iteration 9 completed in 7s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lengstorm Fast Style",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JSNE0OdY4wG2",
        "HttFxLDC2XTL",
        "wtd97bweWlAb",
        "lPaCAOp0DnLp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8ORdd7dU1suw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This is the setup code, must run it before doing anything"
      ]
    },
    {
      "metadata": {
        "id": "ghDweAgurZBM",
        "colab_type": "code",
        "outputId": "14d43498-846e-4cce-e9f3-f4b21193faa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!wget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat -q --show-progress\n",
        "!mv imagenet-vgg-verydeep-19.mat ./data\n",
        "!mkdir -p ./data/bin\n",
        "!mkdir -p inpath\n",
        "!mkdir -p outpath\n",
        "!mkdir -p testdata\n",
        "!curl https://images.wallpaperscraft.com/image/australian_shepherd_dog_aussie_dog_grass_sunset_95327_1920x1080.jpg > dog.jpg\n",
        "!mv dog.jpg ./inpath/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imagenet-vgg-veryde 100%[===================>] 549.36M  67.5MB/s    in 8.5s    \n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  551k  100  551k    0     0   512k      0  0:00:01  0:00:01 --:--:--  512k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PoOMC2WA1yFU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###This code is used for restoring a checkpoint directory from zip file, after uploading it through the left hand panel (files, upload)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bs6e3PbS5CJH",
        "colab_type": "code",
        "outputId": "179cde2b-4e98-4552-d756-85b89abe24ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip checkpoints.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ship_checkpoints.zip\n",
            "   creating: checkpoints/\n",
            "  inflating: checkpoints/fns.ckpt.meta  \n",
            "  inflating: checkpoints/fns.ckpt.index  \n",
            "  inflating: checkpoints/fns.ckpt.data-00000-of-00001  \n",
            "  inflating: checkpoints/checkpoint  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rslPyvFH33-2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are sample transform network checkpoints available here:\n",
        "https://drive.google.com/drive/folders/0B9jhaT37ydSyRk9UX0wwX3BpMzQ"
      ]
    },
    {
      "metadata": {
        "id": "JSNE0OdY4wG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Only run this if training"
      ]
    },
    {
      "metadata": {
        "id": "gh5Ac-rr4vr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bunch of data, be aware \n",
        "!mkdir -p checkpoints\n",
        "!wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip -q --show-progress\n",
        "!mv train2014.zip ./data\n",
        "!unzip -q ./data/train2014.zip\n",
        "#ship training image\n",
        "#~~~~ you can add more training images here ~~~~\n",
        "!curl https://raw.githubusercontent.com/lengstrom/fast-style-transfer/master/examples/style/the_shipwreck_of_the_minotaur.jpg > ship.jpg\n",
        "!mv ship.jpg ./data/\n",
        "#starry night training image\n",
        "!curl https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg > starry.jpg\n",
        "!mv starry.jpg ./data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HttFxLDC2XTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##These are helper flies, you must run them before doing anything"
      ]
    },
    {
      "metadata": {
        "id": "kZIRzsBjriW7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#utils.py\n",
        "import scipy.misc, numpy as np, os, sys\n",
        "\n",
        "def save_img(out_path, img):\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "    scipy.misc.imsave(out_path, img)\n",
        "\n",
        "def scale_img(style_path, style_scale):\n",
        "    scale = float(style_scale)\n",
        "    o0, o1, o2 = scipy.misc.imread(style_path, mode='RGB').shape\n",
        "    scale = float(style_scale)\n",
        "    new_shape = (int(o0 * scale), int(o1 * scale), o2)\n",
        "    style_target = _get_img(style_path, img_size=new_shape)\n",
        "    return style_target\n",
        "\n",
        "def get_img(src, img_size=False):\n",
        "   img = scipy.misc.imread(src, mode='RGB') # misc.imresize(, (256, 256, 3))\n",
        "   if not (len(img.shape) == 3 and img.shape[2] == 3):\n",
        "       img = np.dstack((img,img,img))\n",
        "   if img_size != False:\n",
        "       img = scipy.misc.imresize(img, img_size)\n",
        "   return img\n",
        "\n",
        "def exists(p, msg):\n",
        "    assert os.path.exists(p), msg\n",
        "\n",
        "def list_files(in_path):\n",
        "    files = []\n",
        "    for (dirpath, dirnames, filenames) in os.walk(in_path):\n",
        "        files.extend(filenames)\n",
        "        break\n",
        "\n",
        "    return files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wL1Ud4C_rp2j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#vgg.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import pdb\n",
        "\n",
        "MEAN_PIXEL = np.array([ 123.68 ,  116.779,  103.939])\n",
        "\n",
        "def vnet(data_path, input_image):\n",
        "    layers = (\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "        'relu5_3', 'conv5_4', 'relu5_4'\n",
        "    )\n",
        "\n",
        "    data = scipy.io.loadmat(data_path)\n",
        "    mean = data['normalization'][0][0][0]\n",
        "    mean_pixel = np.mean(mean, axis=(0, 1))\n",
        "    weights = data['layers'][0]\n",
        "\n",
        "    net = {}\n",
        "    current = input_image\n",
        "    for i, name in enumerate(layers):\n",
        "        kind = name[:4]\n",
        "        if kind == 'conv':\n",
        "            kernels, bias = weights[i][0][0][0][0]\n",
        "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
        "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
        "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "            bias = bias.reshape(-1)\n",
        "            current = _Vconv_layer(current, kernels, bias)\n",
        "        elif kind == 'relu':\n",
        "            current = tf.nn.relu(current)\n",
        "        elif kind == 'pool':\n",
        "            current = _pool_layer(current)\n",
        "        net[name] = current\n",
        "\n",
        "    assert len(net) == len(layers)\n",
        "    return net\n",
        "\n",
        "\n",
        "def _Vconv_layer(input, weights, bias):\n",
        "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
        "            padding='SAME')\n",
        "    return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "\n",
        "def _pool_layer(input):\n",
        "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
        "            padding='SAME')\n",
        "\n",
        "\n",
        "def preprocess(image):\n",
        "    return image - MEAN_PIXEL\n",
        "\n",
        "\n",
        "def unprocess(image):\n",
        "    return image + MEAN_PIXEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIS--nx8sBor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#transform.py\n",
        "import tensorflow as tf, pdb\n",
        "\n",
        "WEIGHTS_INIT_STDEV = .1\n",
        "\n",
        "def Tnet(image):\n",
        "    conv1 = _Tconv_layer(image, 32, 9, 1)\n",
        "    conv2 = _Tconv_layer(conv1, 64, 3, 2)\n",
        "    conv3 = _Tconv_layer(conv2, 128, 3, 2)\n",
        "    resid1 = _residual_block(conv3, 3)\n",
        "    resid2 = _residual_block(resid1, 3)\n",
        "    resid3 = _residual_block(resid2, 3)\n",
        "    resid4 = _residual_block(resid3, 3)\n",
        "    resid5 = _residual_block(resid4, 3)\n",
        "    conv_t1 = _conv_tranpose_layer(resid5, 64, 3, 2)\n",
        "    conv_t2 = _conv_tranpose_layer(conv_t1, 32, 3, 2)\n",
        "    conv_t3 = _Tconv_layer(conv_t2, 3, 9, 1, relu=False)\n",
        "    preds = tf.nn.tanh(conv_t3) * 150 + 255./2\n",
        "    return preds\n",
        "\n",
        "def _Tconv_layer(net, num_filters, filter_size, strides, relu=True):\n",
        "    weights_init = _conv_init_vars(net, num_filters, filter_size)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "    net = tf.nn.conv2d(net, weights_init, strides_shape, padding='SAME')\n",
        "    net = _instance_norm(net)\n",
        "    if relu:\n",
        "        net = tf.nn.relu(net)\n",
        "\n",
        "    return net\n",
        "\n",
        "def _conv_tranpose_layer(net, num_filters, filter_size, strides):\n",
        "    weights_init = _conv_init_vars(net, num_filters, filter_size, transpose=True)\n",
        "\n",
        "    batch_size, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
        "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
        "    # new_shape = #tf.pack([tf.shape(net)[0], new_rows, new_cols, num_filters])\n",
        "\n",
        "    new_shape = [batch_size, new_rows, new_cols, num_filters]\n",
        "    tf_shape = tf.stack(new_shape)\n",
        "    strides_shape = [1,strides,strides,1]\n",
        "\n",
        "    net = tf.nn.conv2d_transpose(net, weights_init, tf_shape, strides_shape, padding='SAME')\n",
        "    net = _instance_norm(net)\n",
        "    return tf.nn.relu(net)\n",
        "\n",
        "def _residual_block(net, filter_size=3):\n",
        "    tmp = _Tconv_layer(net, 128, filter_size, 1)\n",
        "    return net + _Tconv_layer(tmp, 128, filter_size, 1, relu=False)\n",
        "\n",
        "def _instance_norm(net, train=True):\n",
        "    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n",
        "    var_shape = [channels]\n",
        "    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n",
        "    shift = tf.Variable(tf.zeros(var_shape))\n",
        "    scale = tf.Variable(tf.ones(var_shape))\n",
        "    epsilon = 1e-3\n",
        "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
        "    return scale * normalized + shift\n",
        "\n",
        "def _conv_init_vars(net, out_channels, filter_size, transpose=False):\n",
        "    _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
        "    if not transpose:\n",
        "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "    else:\n",
        "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "    weights_init = tf.Variable(tf.truncated_normal(weights_shape, stddev=WEIGHTS_INIT_STDEV, seed=1), dtype=tf.float32)\n",
        "    return weights_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xw1x4gLJsKtl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimize.py\n",
        "#has dependencies to transform, vgg, and utils\n",
        "from __future__ import print_function\n",
        "import functools\n",
        "import pdb, time\n",
        "import tensorflow as tf, numpy as np, os\n",
        "\n",
        "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
        "CONTENT_LAYER = 'relu4_2'\n",
        "DEVICES = 'CUDA_VISIBLE_DEVICES'\n",
        "\n",
        "# np arr, np arr\n",
        "def optimize(content_targets, style_target, content_weight, style_weight,\n",
        "             tv_weight, vgg_path, epochs=2, print_iterations=1000,\n",
        "             batch_size=4, save_path='saver/fns.ckpt', slow=False,\n",
        "             learning_rate=1e-3, debug=False):\n",
        "    if slow:\n",
        "        batch_size = 1\n",
        "    mod = len(content_targets) % batch_size\n",
        "    if mod > 0:\n",
        "        print(\"Train set has been trimmed slightly..\")\n",
        "        content_targets = content_targets[:-mod] \n",
        "\n",
        "    style_features = {}\n",
        "\n",
        "    batch_shape = (batch_size,256,256,3)\n",
        "    style_shape = (1,) + style_target.shape\n",
        "    print(style_shape)\n",
        "\n",
        "    # precompute style features\n",
        "    with tf.Graph().as_default(), tf.device('/cpu:0'), tf.Session() as sess:\n",
        "        style_image = tf.placeholder(tf.float32, shape=style_shape, name='style_image')\n",
        "        style_image_pre = preprocess(style_image)\n",
        "        net = vnet(vgg_path, style_image_pre)\n",
        "        style_pre = np.array([style_target])\n",
        "        for layer in STYLE_LAYERS:\n",
        "            features = net[layer].eval(feed_dict={style_image:style_pre})\n",
        "            features = np.reshape(features, (-1, features.shape[3]))\n",
        "            gram = np.matmul(features.T, features) / features.size\n",
        "            style_features[layer] = gram\n",
        "\n",
        "    with tf.Graph().as_default(), tf.Session() as sess:\n",
        "        X_content = tf.placeholder(tf.float32, shape=batch_shape, name=\"X_content\")\n",
        "        X_pre = preprocess(X_content)\n",
        "\n",
        "        # precompute content features\n",
        "        content_features = {}\n",
        "        content_net = vnet(vgg_path, X_pre)\n",
        "        content_features[CONTENT_LAYER] = content_net[CONTENT_LAYER]\n",
        "\n",
        "        if slow:\n",
        "            preds = tf.Variable(\n",
        "                tf.random_normal(X_content.get_shape()) * 0.256\n",
        "            )\n",
        "            preds_pre = preds\n",
        "        else:\n",
        "            preds = Tnet(X_content/255.0)\n",
        "            preds_pre = preprocess(preds)\n",
        "\n",
        "        net = vnet(vgg_path, preds_pre)\n",
        "\n",
        "        content_size = _tensor_size(content_features[CONTENT_LAYER])*batch_size\n",
        "        assert _tensor_size(content_features[CONTENT_LAYER]) == _tensor_size(net[CONTENT_LAYER])\n",
        "        content_loss = content_weight * (2 * tf.nn.l2_loss(\n",
        "            net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) / content_size\n",
        "        )\n",
        "\n",
        "        style_losses = []\n",
        "        for style_layer in STYLE_LAYERS:\n",
        "            layer = net[style_layer]\n",
        "            bs, height, width, filters = map(lambda i:i.value,layer.get_shape())\n",
        "            size = height * width * filters\n",
        "            feats = tf.reshape(layer, (bs, height * width, filters))\n",
        "            feats_T = tf.transpose(feats, perm=[0,2,1])\n",
        "            grams = tf.matmul(feats_T, feats) / size\n",
        "            style_gram = style_features[style_layer]\n",
        "            style_losses.append(2 * tf.nn.l2_loss(grams - style_gram)/style_gram.size)\n",
        "\n",
        "        style_loss = style_weight * functools.reduce(tf.add, style_losses) / batch_size\n",
        "\n",
        "        # total variation denoising\n",
        "        tv_y_size = _tensor_size(preds[:,1:,:,:])\n",
        "        tv_x_size = _tensor_size(preds[:,:,1:,:])\n",
        "        y_tv = tf.nn.l2_loss(preds[:,1:,:,:] - preds[:,:batch_shape[1]-1,:,:])\n",
        "        x_tv = tf.nn.l2_loss(preds[:,:,1:,:] - preds[:,:,:batch_shape[2]-1,:])\n",
        "        tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/batch_size\n",
        "\n",
        "        loss = content_loss + style_loss + tv_loss\n",
        "\n",
        "        # overall loss\n",
        "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        import random\n",
        "        uid = random.randint(1, 100)\n",
        "        print(\"UID: %s\" % uid)\n",
        "        for epoch in range(epochs):\n",
        "            num_examples = len(content_targets)\n",
        "            iterations = 0\n",
        "            while iterations * batch_size < num_examples:\n",
        "                start_time = time.time()\n",
        "                curr = iterations * batch_size\n",
        "                step = curr + batch_size\n",
        "                X_batch = np.zeros(batch_shape, dtype=np.float32)\n",
        "                for j, img_p in enumerate(content_targets[curr:step]):\n",
        "                   X_batch[j] = get_img(img_p, (256,256,3)).astype(np.float32)\n",
        "\n",
        "                iterations += 1\n",
        "                assert X_batch.shape[0] == batch_size\n",
        "\n",
        "                feed_dict = {\n",
        "                   X_content:X_batch\n",
        "                }\n",
        "\n",
        "                train_step.run(feed_dict=feed_dict)\n",
        "                end_time = time.time()\n",
        "                delta_time = end_time - start_time\n",
        "                if debug:\n",
        "                    print(\"UID: %s, batch time: %s\" % (uid, delta_time))\n",
        "                is_print_iter = int(iterations) % print_iterations == 0\n",
        "                if slow:\n",
        "                    is_print_iter = epoch % print_iterations == 0\n",
        "                is_last = epoch == epochs - 1 and iterations * batch_size >= num_examples\n",
        "                should_print = is_print_iter or is_last\n",
        "                if should_print:\n",
        "                    to_get = [style_loss, content_loss, tv_loss, loss, preds]\n",
        "                    test_feed_dict = {\n",
        "                       X_content:X_batch\n",
        "                    }\n",
        "\n",
        "                    tup = sess.run(to_get, feed_dict = test_feed_dict)\n",
        "                    _style_loss,_content_loss,_tv_loss,_loss,_preds = tup\n",
        "                    losses = (_style_loss, _content_loss, _tv_loss, _loss)\n",
        "                    if slow:\n",
        "                       _preds = unprocess(_preds)\n",
        "                    else:\n",
        "                       saver = tf.train.Saver()\n",
        "                       res = saver.save(sess, save_path)\n",
        "                    yield(_preds, losses, iterations, epoch)\n",
        "\n",
        "def _tensor_size(tensor):\n",
        "    from operator import mul\n",
        "    return functools.reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wtd97bweWlAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Below code executes training with input image**"
      ]
    },
    {
      "metadata": {
        "id": "aGh_xL9N9dlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#style.py - to style/train\n",
        "from __future__ import print_function\n",
        "import sys, os, pdb\n",
        "sys.path.insert(0, 'src')\n",
        "import numpy as np, scipy.misc \n",
        "from argparse import ArgumentParser\n",
        "\n",
        "CONTENT_WEIGHT = 7.5e0\n",
        "STYLE_WEIGHT = 1e2\n",
        "TV_WEIGHT = 2e2\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 2\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "CHECKPOINT_ITERATIONS = 2000\n",
        "VGG_PATH = './data/imagenet-vgg-verydeep-19.mat'\n",
        "TRAIN_PATH = './train2014'\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = '/gpu:0'\n",
        "FRAC_GPU = 1\n",
        "TEST_DIR = \"./testdata\"\n",
        "\n",
        "\n",
        "def _get_files(img_dir):\n",
        "    files = list_files(img_dir)\n",
        "    return [os.path.join(img_dir,x) for x in files]\n",
        "\n",
        "    \n",
        "def main():\n",
        "\n",
        "    stylepath = input(\"Path to style image: \")\n",
        "    #trainpath = input(\"Path to training images folder: \")\n",
        "    trainpath = TRAIN_PATH\n",
        "    #checkpointpath = input(\"Directory to save checkpoint in: \")\n",
        "    checkpointpath = CHECKPOINT_DIR\n",
        "    #vggpath = input(\"Path to vgg file (check /data): \")\n",
        "    vggpath = VGG_PATH\n",
        "    test_dir = TEST_DIR\n",
        "    \n",
        "    style_target = get_img(stylepath)\n",
        "    #if not options.slow:\n",
        "        #content_targets = _get_files(options.train_path)\n",
        "    content_targets = _get_files(trainpath)\n",
        "    #elif options.test:\n",
        "    #    content_targets = [options.test]\n",
        "\n",
        "    kwargs = {\n",
        "        #\"slow\":options.slow,\n",
        "        #\"epochs\":options.epochs,\n",
        "        \"epochs\":NUM_EPOCHS,\n",
        "        #\"print_iterations\":options.checkpoint_iterations,\n",
        "        \"print_iterations\":CHECKPOINT_ITERATIONS,\n",
        "        #\"batch_size\":options.batch_size,\n",
        "        \"batch_size\":BATCH_SIZE,\n",
        "        #\"save_path\":os.path.join(options.checkpoint_dir,'fns.ckpt'),\n",
        "        \"save_path\":os.path.join(checkpointpath,'fns.ckpt'),\n",
        "        #\"learning_rate\":options.learning_rate\n",
        "        \"learning_rate\":LEARNING_RATE\n",
        "    }\n",
        "    \n",
        "    #if options.slow:\n",
        "    #    if options.epochs < 10:\n",
        "    #        kwargs['epochs'] = 1000\n",
        "    #    if options.learning_rate < 1:\n",
        "    #        kwargs['learning_rate'] = 1e1\n",
        "\n",
        "    args = [\n",
        "        content_targets,\n",
        "        style_target,\n",
        "        #options.content_weight,\n",
        "        CONTENT_WEIGHT,\n",
        "        #options.style_weight,\n",
        "        STYLE_WEIGHT,\n",
        "        #options.tv_weight,\n",
        "        TV_WEIGHT,\n",
        "        #options.vgg_path\n",
        "        vggpath\n",
        "    ]\n",
        "\n",
        "    for preds, losses, i, epoch in optimize(*args, **kwargs):\n",
        "        style_loss, content_loss, tv_loss, loss = losses\n",
        "\n",
        "        print('Epoch %d, Iteration: %d, Loss: %s' % (epoch, i, loss))\n",
        "        to_print = (style_loss, content_loss, tv_loss)\n",
        "        print('style: %s, content:%s, tv: %s' % to_print)\n",
        "        #preds_path = '%s/%s_%s.png' % (test_dir,epoch,i)\n",
        "        #if options.test:\n",
        "         #   assert options.test_dir != False\n",
        "          #  preds_path = '%s/%s_%s.png' % (options.test_dir,epoch,i)\n",
        "           # if not options.slow:\n",
        "            #    ckpt_dir = os.path.dirname(options.checkpoint_dir)\n",
        "             #   evaluate.ffwd_to_img(options.test,preds_path,\n",
        "              #                       options.checkpoint_dir)\n",
        "           # else:\n",
        "              #save_img(preds_path, img)\n",
        "    #ckpt_dir = options.checkpoint_dir\n",
        "    ckpt_dir = checkpointpath\n",
        "    cmd_text = 'python evaluate.py --checkpoint %s ...' % ckpt_dir\n",
        "    print(\"Training complete. For evaluation:\\n    `%s`\" % cmd_text)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPaCAOp0DnLp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#This section applies the style transfer to an image it grabs from an Ngrok address\n",
        "\n",
        "I've commented out the ngrok stuff that was used for the demo, you ***must*** upload a sample image to ./inpath/ and the results will be given in ./outpath/"
      ]
    },
    {
      "metadata": {
        "id": "bt8PlMPMxq-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evaluate.py\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "sys.path.insert(0, 'src')\n",
        "import numpy as np, pdb, os\n",
        "import scipy.misc\n",
        "import tensorflow as tf\n",
        "from argparse import ArgumentParser\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import json\n",
        "import subprocess\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "import moviepy.video.io.ffmpeg_writer as ffmpeg_writer\n",
        "from IPython.display import Image, display, clear_output\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "IN_PATH = './inpath'\n",
        "OUT_PATH = './outpath'\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = '/gpu:0'\n",
        "\n",
        "\n",
        "def ffwd_video(path_in, path_out, checkpoint_dir, device_t='/gpu:0', batch_size=4):\n",
        "    video_clip = VideoFileClip(path_in, audio=False)\n",
        "    video_writer = ffmpeg_writer.FFMPEG_VideoWriter(path_out, video_clip.size, video_clip.fps, codec=\"libx264\",\n",
        "                                                    preset=\"medium\", bitrate=\"2000k\",\n",
        "                                                    audiofile=path_in, threads=None,\n",
        "                                                    ffmpeg_params=None)\n",
        "\n",
        "    g = tf.Graph()\n",
        "    soft_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "    soft_config.gpu_options.allow_growth = True\n",
        "    with g.as_default(), g.device(device_t), \\\n",
        "            tf.Session(config=soft_config) as sess:\n",
        "        batch_shape = (batch_size, video_clip.size[1], video_clip.size[0], 3)\n",
        "        img_placeholder = tf.placeholder(tf.float32, shape=batch_shape,\n",
        "                                         name='img_placeholder')\n",
        "\n",
        "        preds = Tnet(img_placeholder)\n",
        "        saver = tf.train.Saver()\n",
        "        if os.path.isdir(checkpoint_dir):\n",
        "            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            else:\n",
        "                raise Exception(\"No checkpoint found...\")\n",
        "        else:\n",
        "            saver.restore(sess, checkpoint_dir)\n",
        "\n",
        "        X = np.zeros(batch_shape, dtype=np.float32)\n",
        "\n",
        "        def style_and_write(count):\n",
        "            for i in range(count, batch_size):\n",
        "                X[i] = X[count - 1]  # Use last frame to fill X\n",
        "            _preds = sess.run(preds, feed_dict={img_placeholder: X})\n",
        "            for i in range(0, count):\n",
        "                video_writer.write_frame(np.clip(_preds[i], 0, 255).astype(np.uint8))\n",
        "\n",
        "        frame_count = 0  # The frame count that written to X\n",
        "        for frame in video_clip.iter_frames():\n",
        "            X[frame_count] = frame\n",
        "            frame_count += 1\n",
        "            if frame_count == batch_size:\n",
        "                style_and_write(frame_count)\n",
        "                frame_count = 0\n",
        "\n",
        "        if frame_count != 0:\n",
        "            style_and_write(frame_count)\n",
        "\n",
        "        video_writer.close()\n",
        "\n",
        "\n",
        "# get img_shape\n",
        "def ffwd(data_in, paths_out, checkpoint_dir, device_t='/gpu:0', batch_size=4):\n",
        "    assert len(paths_out) > 0\n",
        "    is_paths = type(data_in[0]) == str\n",
        "    if is_paths:\n",
        "        assert len(data_in) == len(paths_out)\n",
        "        img_shape = get_img(data_in[0]).shape\n",
        "    else:\n",
        "        assert data_in.size[0] == len(paths_out)\n",
        "        img_shape = X[0].shape\n",
        "\n",
        "    g = tf.Graph()\n",
        "    batch_size = min(len(paths_out), batch_size)\n",
        "    curr_num = 0\n",
        "    soft_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "    soft_config.gpu_options.allow_growth = True\n",
        "    with g.as_default(), g.device(device_t), \\\n",
        "            tf.Session(config=soft_config) as sess:\n",
        "        batch_shape = (batch_size,) + img_shape\n",
        "        img_placeholder = tf.placeholder(tf.float32, shape=batch_shape,\n",
        "                                         name='img_placeholder')\n",
        "\n",
        "        preds = Tnet(img_placeholder)\n",
        "        saver = tf.train.Saver()\n",
        "        if os.path.isdir(checkpoint_dir):\n",
        "            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            else:\n",
        "                raise Exception(\"No checkpoint found...\")\n",
        "        else:\n",
        "            saver.restore(sess, checkpoint_dir)\n",
        "\n",
        "        num_iters = int(len(paths_out)/batch_size)\n",
        "        for i in range(num_iters):\n",
        "            pos = i * batch_size\n",
        "            curr_batch_out = paths_out[pos:pos+batch_size]\n",
        "            if is_paths:\n",
        "                curr_batch_in = data_in[pos:pos+batch_size]\n",
        "                X = np.zeros(batch_shape, dtype=np.float32)\n",
        "                for j, path_in in enumerate(curr_batch_in):\n",
        "                    img = get_img(path_in)\n",
        "                    assert img.shape == img_shape, \\\n",
        "                        'Images have different dimensions. ' +  \\\n",
        "                        'Resize images or use --allow-different-dimensions.'\n",
        "                    X[j] = img\n",
        "            else:\n",
        "                X = data_in[pos:pos+batch_size]\n",
        "\n",
        "            _preds = sess.run(preds, feed_dict={img_placeholder:X})\n",
        "            for j, path_out in enumerate(curr_batch_out):\n",
        "                save_img(path_out, _preds[j])\n",
        "                \n",
        "        remaining_in = data_in[num_iters*batch_size:]\n",
        "        remaining_out = paths_out[num_iters*batch_size:]\n",
        "    if len(remaining_in) > 0:\n",
        "        ffwd(remaining_in, remaining_out, checkpoint_dir, \n",
        "            device_t=device_t, batch_size=1)\n",
        "\n",
        "def ffwd_to_img(in_path, out_path, checkpoint_dir, device='/cpu:0'):\n",
        "    paths_in, paths_out = [in_path], [out_path]\n",
        "    ffwd(paths_in, paths_out, checkpoint_dir, batch_size=1, device_t=device)\n",
        "\n",
        "def ffwd_different_dimensions(in_path, out_path, checkpoint_dir, \n",
        "            device_t=DEVICE, batch_size=4):\n",
        "    in_path_of_shape = defaultdict(list)\n",
        "    out_path_of_shape = defaultdict(list)\n",
        "    for i in range(len(in_path)):\n",
        "        in_image = in_path[i]\n",
        "        out_image = out_path[i]\n",
        "        shape = \"%dx%dx%d\" % get_img(in_image).shape\n",
        "        in_path_of_shape[shape].append(in_image)\n",
        "        out_path_of_shape[shape].append(out_image)\n",
        "    for shape in in_path_of_shape:\n",
        "        print('Processing images of shape %s' % shape)\n",
        "        ffwd(in_path_of_shape[shape], out_path_of_shape[shape], \n",
        "            checkpoint_dir, device_t, batch_size)\n",
        "\n",
        "def main():\n",
        "    in_path = IN_PATH\n",
        "    out_path = OUT_PATH\n",
        "    checkpoint_dir = CHECKPOINT_DIR\n",
        "    batch_size = BATCH_SIZE\n",
        "    device = DEVICE\n",
        "    allow_different_dimensions = True\n",
        "    #ngrok = input(\"Ngrok address: \")\n",
        "    #device = input(\"Input device serial: \")\n",
        "    #print(ngrok)\n",
        "    i = 0\n",
        "    while (i < 1):\n",
        "      ts = time.time()\n",
        "      int(ts)\n",
        "      #!curl -s \"http://\"{ngrok}\"/\"{device}\"/screen.png?d=\"{ts} > screen.jpg\n",
        "      #!curl -s \"http://\"{ngrok}\"/a8786d9d/screen.jpg?d=\"{ts} > screen.jpg\n",
        "      !mv screen.jpg ./inpath\n",
        "      if not os.path.isdir(in_path):\n",
        "        if os.path.exists(out_path) and os.path.isdir(out_path):\n",
        "            out_path = \\\n",
        "                    os.path.join(out_path,os.path.basename(in_path))\n",
        "        else:\n",
        "            out_path = out_path\n",
        "\n",
        "        ffwd_to_img(in_path, out_path, checkpoint_dir,\n",
        "                    device=opts.device)\n",
        "      else:\n",
        "        files = list_files(in_path)\n",
        "        full_in = [os.path.join(in_path,x) for x in files]\n",
        "        full_out = [os.path.join(out_path,x) for x in files]\n",
        "        if allow_different_dimensions:\n",
        "          try:\n",
        "            ffwd_different_dimensions(full_in, full_out, checkpoint_dir, \n",
        "                    device_t=device, batch_size=batch_size)\n",
        "          except OSError:\n",
        "            print(\"errored out, trying again\")\n",
        "        else :\n",
        "          ffwd(full_in, full_out, checkpoint_dir, device,\n",
        "                    batch_size)\n",
        "      #clear_output()\n",
        "      #display(Image('./outpath/screen.jpg'))\n",
        "      i = i+1\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mkw5qw2i3WHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to save transform network, run this command and then don't forget to download the file from the lefthand side"
      ]
    },
    {
      "metadata": {
        "id": "9V36-tLgKCoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Save checkpoint data\n",
        "!ls -lah ./checkpoints/\n",
        "!zip -r checkpoints.zip ./checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}